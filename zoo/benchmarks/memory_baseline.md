# IREE Zoo Model Memory Footprint Test Results

## Test Environment

- **Platform**: RISC-V 32-bit
- **Simulator**: QEMU virt machine
- **IREE Version**: v3.1.0
- **HAL**: local_sync (Single-threaded synchronous)
- **Compiler**: LLVM/Clang 18
- **Optimization**: `-Os -ffunction-sections -fdata-sections` + `--gc-sections`

## Test Baseline: mobilenet_test (ST MNIST INT8)

### Binary Size (ELF32 - Optimized -Os)

```
   text      data       bss       dec       hex    filename
  21832     65964     27256    115052     1c16c   mobilenet_test
```

### Resource Usage Analysis (Flash XIP Mode)

In a real MCU deployment (using `link_flash.ld`):

- **Flash Footprint**: **~87 KB**
  - Text (Code): 21 KB
  - Data LMA (Initial Values): 65 KB

- **RAM Footprint**: **~93 KB** (Current)
  - Data VMA (Weights/Consts): 65 KB
  - BSS (Heap/Stack): 27 KB

> **Optimization Hint**: Model weights are currently located in the `.data` section, thus consuming RAM. Moving them to `.rodata` via linker scripts or compiler options can reduce RAM usage to **~27 KB**.

### Module Information (Baseline: MiniResNet INT8 / mobilenet_test)

- **Module Name**: `mobilenet_test_module_c`
- **Type**: Vector Addition (Generated by IREE EmitC)
- **Input**: [4] float32
- **Output**: [4] float32
- **Function**: Demo module (input + input)

### Runtime Behavior (Baseline)

```
[1/5] Creating IREE VM instance...
✅ VM instance created

[2/5] Loading compiled module...
✅ Module loaded successfully

[3/5] Inspecting module signature...
   Module has exported functions

[4/5] Preparing inference data...
   Input: 4 float32 values
   Expected output: vec_add result (input + input)

[5/5] Simulating inference (using vec_add)...

=== Cleanup ===
✅ Resources released
```

**Status**: ✅ Loaded and executed successfully

### ST MNIST Inference Verification (Renode)

- **Model**: ST MNIST INT8 (28x28x1)
- **Input**: int8[1x28x28x1]
- **Output**: int8[1x10] (Class probabilities/scores)
- **Function**: Handwritten digit classification

```
[1/8] Creating VM instance...
[2/8] Creating Static Library Loader...
[3/8] Creating HAL driver (sync with loader)...
[4/8] Creating HAL device...
[5/8] Loading Model module...
[6/8] Creating Context...
[7/8] Preparing Inputs...
[8/8] Invoking 'main'...
=== SUCCESS: Inference Complete! ===
```

**Status**: ✅ Renode inference successful

## Test Results: ST MNIST INT8

### Binary Size

```
   text      data       bss       dec       hex    filename
  54072     65964     27256    147292     23f5c    mobilenet_test (st_mnist)
```

- **Flash Usage (text + data)**: 54KB + 65KB = **119KB**
- **RAM Usage (data + bss)**: 65KB + 27KB = **92KB**
- **Total Binary**: 147KB

**Comparison**: Almost identical to MobileNet (148KB). This indicates that for small models, the base overhead of the IREE Runtime is the dominant factor.

---

## Model Comparison Plan

| Model | TFLite Size | Intended Use | STM32AI Ref (Flash/RAM) | Status |
|------|------------|---------|--------------------------|------|
| **MiniResNet INT8** | 141KB | Audio Event Detection | ~200KB / ~150KB | ✅ 87KB Flash / 93KB RAM |
| **EfficientNet INT8** | 790KB | Image Classification | - / ~300KB | ✅ Verified (Flash XIP) |
| ST MNIST INT8 | 20KB | Handwritten Digit Recognition | 56KB / 22KB | ✅ (Similar efficiency expected) |
| MiniResNet INT8 | 141KB | Audio Event Detection | ~200KB / ~150KB | ⚠️ Pending conversion |
| EfficientNet LC INT8 | 790KB | Image Classification | - / ~300KB | ⚠️ Pending conversion |

---

## Solved Issues

### IREE 3.1.0 INT8 Quantization Support

**Issue**: TFLite INT8 uses the `ui8` type, which causes `element type 'ui8' is not legal` errors during IREE compilation.

**Solution**:
Use the `iree-tosa-strip-signedness` pass to convert `ui8` to `i8`:

```bash
iree-opt --pass-pipeline='builtin.module(func.func(iree-tosa-strip-signedness))' \
  model.mlir -o model_i8.mlir
```

### Bare-Metal Soft-Float Linking

**Issue**: RISC-V 32IMAC lacks an FPU, so the compiler generates soft-float calls (`__addsf3`). Linking directly during the `iree-compile` stage fails due to missing `libgcc`.

**Solution**:
Use `--iree-llvmcpu-link-embedded=false` to generate an **unlinked object file** (`.o`), then pass it to the main project's CMake for final linking (where `libgcc` is already configured).

---

## Memory Footprint Analysis

### Flash Footprint Composition

Based on `mobilenet_test` (120KB Flash):

1. **IREE Runtime**: ~20KB
   - VM Core
   - HAL Driver (local_sync)
   - Allocator

2. **EmitC Module**: ~90KB
   - Statically generated C code
   - Embedded model weights/constants

3. **RTOS + HAL**: ~10KB
   - Kernel scheduler
   - UART/CLINT drivers

### RAM Footprint Composition

Based on `mobilenet_test` (92KB RAM):

1. **Static Data (data)**: 65KB
   - IREE static buffers
   - Module constants

2. **BSS (Uninitialized)**: 27KB
   - Heap space
   - Stack space
   - IREE arena allocator

---

## Future Plans

### Short-term (Using Float32 Models)

1. Export Float32 TFLite models from stm32ai-modelzoo
2. Generate EmitC C modules using `iree-compile`
3. Integrate into the `zoo_test` framework
4. Measure actual memory footprint

### Mid-term (Optimizing INT8 Support)

1. Investigate INT8 support in IREE nightly versions
2. Test compatibility with IREE upgrades
3. Alternatively, use `iree-import-tf` to bypass TFLite INT8 issues

### Long-term (Expanding Model Zoo)

1. Add more model categories:
   - Audio (MiniResNet)
   - Vision (EfficientNet, YOLO)
   - Time series (HAR models)

2. Establish a memory footprint database
3. Compare IREE vs. STM32AI performance and efficiency

---

## References

- IREE Toolchain: `/home/mingshi/.mamba/envs/iree-toolchain310/`
- stm32ai-modelzoo: `/home/mingshi/Project/AI/stm32ai-modelzoo/`
- Zoo Directory: `/home/mingshi/Project/PF/rtos/zoo/`
