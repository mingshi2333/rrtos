# MNIST App - AI Inference Example

This application is automatically generated by the `ai_codegen.py` tool. It demonstrates how to integrate a TFLite model into RRTOS using the IREE runtime with a structured, easy-to-use application scaffold.

## Project Structure

- `CMakeLists.txt`: Configures the build, linking against the AI model library and RRTOS kernel.
- `src/main.c`: The main entry point. It initializes hardware, the RRTOS kernel, and a dedicated AI task that runs the inference loop.
- `src/hooks.c`: **User customization area**. Contains templates for input pre-processing and output post-processing.
- `generated/`: Contains the machine-generated C registry and the compiled model object files.

## How to Customize

### 1. Pre-processing (`src/hooks.c`)
Implement `ai_app_pre_process` to handle data preparation, such as image normalization or scaling.
```c
void ai_app_pre_process(void *input_data, size_t size) {
    // Cast input_data to ai_st_mnist_28_input_t* if needed
    // Perform normalization or copy sensor data here
}
```

### 2. Post-processing (`src/hooks.c`)
Implement `ai_app_post_process` to interpret the model's output, such as finding the class with the highest probability (ArgMax).
```c
void ai_app_post_process(void *output_data, size_t size) {
    // Cast output_data to ai_st_mnist_28_output_t*
    // Find the max index in the probabilities array
}
```

## Building and Running

### Build the application
From the project root:
```bash
pixi run -e rv32 build
```

### Run in Simulation
Run using QEMU:
```bash
qemu-system-riscv32 -machine virt -nographic -m 4M -bios none -kernel build/apps/mnist_app/mnist_app
```

## Key APIs

- `ai_runtime_init()`: Initializes the AI model registry. Must be called before any inference.
- `ai_st_mnist_28_run()`: Performs a synchronous inference using the model.
- `os_task_delay()`: Yields the CPU to other tasks in the RTOS.
