# AI Demo - Lightweight AI Inference Template

`ai_demo` provides a STM32CubeAI-style lightweight inference framework, designed to offer embedded developers the simplest API to deploy IREE-compiled models on AIOS.

## ğŸ“ File Structure

- `ai_model.h`: **User Configuration**. Define model name, input/output dimensions, and data types.
- `ai_model.c`: **Model Wrapper**. Handles interaction with the IREE runtime; usually requires no modification (unless changing library query function names).
- `main.c`: **Application Logic**. Demonstrates how to initialize the runtime, prepare data, and execute inference.
- `CMakeLists.txt`: **Build Configuration**. Links the model's static library (.o file).

## ğŸš€ Quick Start

### 1. Configure Model (`ai_model.h`)
Modify the following macros based on your model parameters:

```c
/* Model display name */
#define AI_MODEL_NAME           "my_model"

/* Input tensor configuration (NHWC) */
#define AI_INPUT_HEIGHT         28
#define AI_INPUT_WIDTH          28
#define AI_INPUT_CHANNELS       1

/* Number of output classes */
#define AI_OUTPUT_CLASSES       10

/* Data types: Supports int8_t, uint8_t, float */
typedef int8_t  ai_input_t;
typedef int8_t  ai_output_t;
```

### 2. Link Model Static Library
Place the IREE-generated `.o` file in the project and modify the external symbols in `ai_model.c`:

```c
/* 1. Ensure EmitC module creation function exists (usually generated by IREE) */
extern iree_status_t module_create(...);

/* 2. Update static library query function name (must match model name) */
extern const iree_hal_executable_library_header_t**
    your_model_library_query(...);
```

### 3. Application Integration (`main.c`)
Call three core APIs:

```c
// 1. Initialization
ai_init();

// 2. Execution
ai_run(input_data);

// 3. Get Prediction
int result = ai_get_prediction(&confidence);
```

## ğŸ”„ Model Update Workflow

1. **Export Model**: Compile the model using IREE into an EmitC static library (generates `.h` and `.o`).
2. **Update Header**: Update dimension parameters in `ai_model.h`.
3. **Bind Symbols**: Update the `*_library_query` function declaration and call in `ai_model.c`.
4. **CMake Linking**: Add the new `.o` file to the link list in `CMakeLists.txt`.

## ğŸ“ Developer Notes

### Symbol Dependencies
- **fmaximumf/fminimumf**: Code generated by IREE may depend on C23 standard `fmaximumf`. In the `picolibc` environment, these symbols are provided by `hal/src/math_compat.c`; ensure this file is included in the build.
- **Library Query**: Each IREE-generated model has a unique `_library_query` entry point, which must be correctly registered in the `libraries` array in `ai_model.c`.

### Performance and Memory
- Quantized `int8_t` models are recommended for optimal performance.
- Inference tasks typically require significant stack space (recommend `ai_stack` > 32KB).

## ğŸ› ï¸ Build and Run

```bash
# Enter build directory and configure (using preset environment)
pixi run -e rv32 configure

# Build demo
pixi run -e rv32 build

# Run Renode simulation
pixi run -e rv32 renode --disable-xwt -e \
    "$bin=@build/apps/ai_demo/ai_demo.elf; s @scripts/simulation/mobilenet_test_rv32.resc; start"
```